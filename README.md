このNode.jsスクリプトは、指定されたURLから始まるウェブサイト全体をクロールし、そのサイト内の\*\*リンク切れ（404エラー）\*\*を効率的に見つけ出すツールです。見つかったリンク切れは、コンソールに出力されるだけでなく、詳細情報が記載されたCSVファイルとして保存されます。

-----

## 機能と特徴

  * **ページと画像のリンク切れ検出**: ページへのリンクだけでなく、ページ内に埋め込まれた画像のリンク切れもチェックします。
  * **シンプルな設定**: クロールを開始するURLと、結果を保存するCSVファイルのパスを簡単に設定できます。
  * **再帰的クロール**: 指定された基点URLから始まる内部リンクを自動的に辿り、サイト全体を網羅的に検査します。
  * **重複チェック**: すでにクロールしたURLや、キューに追加済みのURLはスキップするため、無駄なリクエストを防ぎます。
  * **詳細なレポート**: リンク切れが見つかった場合、その種類（ページ/画像）、問題のURL、リンクが見つかった元のページURL、ページのタイトルをコンソールとCSVファイルで確認できます。

-----

## 🚀 はじめかた

### 1\. 必要なパッケージのインストール

このスクリプトは、HTTPリクエストに`axios`、HTMLの解析に`jsdom`、CSVファイルの書き出しに`fast-csv`を使用します。以下のコマンドでまとめてインストールできます。

```bash
npm install axios jsdom fast-csv
```

### 2\. 設定の変更

スクリプトの先頭にある**設定項目**を、ご自身の環境に合わせて変更してください。

```javascript
// --- 設定項目 ---
// クロールを開始する基点URLを指定。
const START_URL = 'https://example.com/about/'; 
// 結果を出力するCSVファイルのパスを指定
const OUTPUT_CSV_FILE = './all_broken_links.csv';
// ----------------
```

  * `START_URL`: **クロールを開始したいURL**を設定します。`https://your-site.com/` のように、末尾にスラッシュを付けることを推奨します。これにより、同じディレクトリ内のページが正しく認識されます。
  * `OUTPUT_CSV_FILE`: リンク切れ情報を保存する**CSVファイルの名前とパス**を指定します。

### 3\. スクリプトの実行

設定が完了したら、以下のコマンドでスクリプトを実行します。

```bash
node checkImage.js
```

実行中、クロールしているURLや見つかったリンク切れの情報がリアルタイムでコンソールに表示されます。

-----

## 📝 結果の確認

スクリプトが完了すると、`OUTPUT_CSV_FILE`で指定したパスに\*\*`all_broken_links.csv`\*\*ファイルが作成されます。このファイルには、以下の列が含まれています。

  * `Type`: リンクの種類（`Page` または `Image`）
  * `Broken_URL`: リンク切れとなっているURL
  * `Found_On_Page`: そのリンク切れが見つかったページのURL
  * `Page_Title`: そのページのタイトル
